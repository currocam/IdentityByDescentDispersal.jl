@article{ringbauer_inferring_2017,
	title = {Inferring {Recent} {Demography} from {Isolation} by {Distance} of {Long} {Shared} {Sequence} {Blocks}},
	volume = {205},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5340342/},
	doi = {10.1534/genetics.116.196220},
	abstract = {Recently it has become feasible to detect long blocks of nearly identical sequence shared between pairs of genomes. These identity-by-descent (IBD) blocks are direct traces of recent coalescence events and, as such, contain ample signal to infer recent demography. Here, we examine sharing of such blocks in two-dimensional populations with local migration. Using a diffusion approximation to trace genetic ancestry, we derive analytical formulas for patterns of isolation by distance of IBD blocks, which can also incorporate recent population density changes. We introduce an inference scheme that uses a composite-likelihood approach to fit these formulas. We then extensively evaluate our theory and inference method on a range of scenarios using simulated data. We first validate the diffusion approximation by showing that the theoretical results closely match the simulated block-sharing patterns. We then demonstrate that our inference scheme can accurately and robustly infer dispersal rate and effective density, as well as bounds on recent dynamics of population density. To demonstrate an application, we use our estimation scheme to explore the fit of a diffusion model to Eastern European samples in the Population Reference Sample data set. We show that ancestry diffusing with a rate of σ≈50−−100 km/gen during the last centuries, combined with accelerating population growth, can explain the observed exponential decay of block sharing with increasing pairwise sample distance.},
	number = {3},
	urldate = {2024-05-05},
	journal = {Genetics},
	author = {Ringbauer, Harald and Coop, Graham and Barton, Nicholas H.},
	month = mar,
	year = {2017},
	pmid = {28108588},
	pmcid = {PMC5340342},
	pages = {1335--1351},
}

@article{haller_slim_2023,
	title = {{SLiM} 4: {Multispecies} {Eco}-{Evolutionary} {Modeling}},
	volume = {201},
	issn = {0003-0147},
	shorttitle = {{SLiM} 4},
	url = {https://www.journals.uchicago.edu/doi/10.1086/723601},
	doi = {10.1086/723601},
	abstract = {The SLiM software framework for genetically explicit forward simulation has been widely used in population genetics. However, it has been largely restricted to modeling only a single species, which has limited its broader utility in evolutionary biology. Indeed, to our knowledge no general-purpose, flexible modeling framework exists that provides support for simulating multiple species while also providing other key features, such as explicit genetics and continuous space. The lack of such software has limited our ability to model higher biological levels such as communities, ecosystems, coevolutionary and eco-evolutionary processes, and biodiversity, which is crucial for many purposes, from extending our basic understanding of evolutionary ecology to informing conservation and management decisions. We here announce the release of SLiM 4, which fills this important gap by adding support for multiple species, including ecological interactions between species such as predation, parasitism, and mutualism, and illustrate its new features with examples.},
	number = {5},
	urldate = {2024-12-05},
	journal = {The American Naturalist},
	author = {Haller, Benjamin C. and Messer, Philipp W.},
	month = may,
	year = {2023},
	note = {Publisher: The University of Chicago Press},
	keywords = {simulation, biodiversity, coevolution, community, evolutionary ecology, multispecies},
	pages = {E127--E139},
}

@article{baumdicker_efficient_2022,
	title = {Efficient ancestry and mutation simulation with msprime 1.0},
	volume = {220},
	issn = {1943-2631},
	url = {https://doi.org/10.1093/genetics/iyab229},
	doi = {10.1093/genetics/iyab229},
	abstract = {Stochastic simulation is a key tool in population genetics, since the models involved are often analytically intractable and simulation is usually the only way of obtaining ground-truth data to evaluate inferences. Because of this, a large number of specialized simulation programs have been developed, each filling a particular niche, but with largely overlapping functionality and a substantial duplication of effort. Here, we introduce msprime version 1.0, which efficiently implements ancestry and mutation simulations based on the succinct tree sequence data structure and the tskit library. We summarize msprime’s many features, and show that its performance is excellent, often many times faster and more memory efficient than specialized alternatives. These high-performance features have been thoroughly tested and validated, and built using a collaborative, open source development model, which reduces duplication of effort and promotes software quality via community engagement.},
	number = {3},
	urldate = {2024-06-06},
	journal = {Genetics},
	author = {Baumdicker, Franz and Bisschop, Gertjan and Goldstein, Daniel and Gower, Graham and Ragsdale, Aaron P and Tsambos, Georgia and Zhu, Sha and Eldon, Bjarki and Ellerman, E Castedo and Galloway, Jared G and Gladstein, Ariella L and Gorjanc, Gregor and Guo, Bing and Jeffery, Ben and Kretzschumar, Warren W and Lohse, Konrad and Matschiner, Michael and Nelson, Dominic and Pope, Nathaniel S and Quinto-Cortés, Consuelo D and Rodrigues, Murillo F and Saunack, Kumar and Sellinger, Thibaut and Thornton, Kevin and van Kemenade, Hugo and Wohns, Anthony W and Wong, Yan and Gravel, Simon and Kern, Andrew D and Koskela, Jere and Ralph, Peter L and Kelleher, Jerome},
	month = mar,
	year = {2022},
	pages = {iyab229},
}

@article{haller_tree-sequence_2019,
	title = {Tree-sequence recording in {SLiM} opens new horizons for forward-time simulation of whole genomes},
	volume = {19},
	issn = {1755-098X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6393187/},
	doi = {10.1111/1755-0998.12968},
	number = {2},
	urldate = {2025-02-05},
	journal = {Molecular ecology resources},
	author = {Haller, Benjamin C. and Galloway, Jared and Kelleher, Jerome and Messer, Philipp W. and Ralph, Peter L.},
	month = mar,
	year = {2019},
	pmid = {30565882},
	pmcid = {PMC6393187},
	pages = {552--566},
}

@misc{geoga_fitting_2022,
	title = {Fitting {Matérn} {Smoothness} {Parameters} {Using} {Automatic} {Differentiation}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2201.00090},
	doi = {10.48550/ARXIV.2201.00090},
	abstract = {The Matérn covariance function is ubiquitous in the application of Gaussian processes to spatial statistics and beyond. Perhaps the most important reason for this is that the smoothness parameter \$ν\$ gives complete control over the mean-square differentiability of the process, which has significant implications for the behavior of estimated quantities such as interpolants and forecasts. Unfortunately, derivatives of the Matérn covariance function with respect to \$ν\$ require derivatives of the modified second-kind Bessel function \${\textbackslash}mathcal\{K\}\_ν\$ with respect to \$ν\$. While closed form expressions of these derivatives do exist, they are prohibitively difficult and expensive to compute. For this reason, many software packages require fixing \$ν\$ as opposed to estimating it, and all existing software packages that attempt to offer the functionality of estimating \$ν\$ use finite difference estimates for \${\textbackslash}partial\_ν{\textbackslash}mathcal\{K\}\_ν\$. In this work, we introduce a new implementation of \${\textbackslash}mathcal\{K\}\_ν\$ that has been designed to provide derivatives via automatic differentiation (AD), and whose resulting derivatives are significantly faster and more accurate than those computed using finite differences. We provide comprehensive testing for both speed and accuracy and show that our AD solution can be used to build accurate Hessian matrices for second-order maximum likelihood estimation in settings where Hessians built with finite difference approximations completely fail.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Geoga, Christopher J. and Marin, Oana and Schanen, Michel and Stein, Michael L.},
	year = {2022},
	note = {Version Number: 3},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, Methodology (stat.ME)},
}

@article{smith_dispersenn2_2023,
	title = {{disperseNN2}: a neural network for estimating dispersal distance from georeferenced polymorphism data},
	volume = {24},
	issn = {1471-2105},
	shorttitle = {{disperseNN2}},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-023-05522-7},
	doi = {10.1186/s12859-023-05522-7},
	abstract = {Abstract

              Spatial genetic variation is shaped in part by an organism’s dispersal ability. We present a deep learning tool, , for estimating the mean per-generation dispersal distance from georeferenced polymorphism data. Our neural network performs feature extraction on pairs of genotypes, and uses the geographic information that comes with each sample. These attributes led  to outperform a state-of-the-art deep learning method that does not use explicit spatial information: the mean relative absolute error was reduced by 33\% and 48\% using sample sizes of 10 and 100 individuals, respectively.  is particularly useful for non-model organisms or systems with sparse genomic resources, as it uses unphased, single nucleotide polymorphisms as its input. The software is open source and available from
              https://github.com/kr-colab/disperseNN2
              , with documentation located at
              https://dispersenn2.readthedocs.io/en/latest/
              .},
	language = {en},
	number = {1},
	urldate = {2025-06-27},
	journal = {BMC Bioinformatics},
	author = {Smith, Chris C. R. and Kern, Andrew D.},
	month = oct,
	year = {2023},
	pages = {385},
}

@article{driscoll_trajectory_2014,
	title = {The {Trajectory} of {Dispersal} {Research} in {Conservation} {Biology}. {Systematic} {Review}},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095053},
	doi = {10.1371/journal.pone.0095053},
	abstract = {Dispersal knowledge is essential for conservation management, and demand is growing. But are we accumulating dispersal knowledge at a pace that can meet the demand? To answer this question we tested for changes in dispersal data collection and use over time. Our systematic review of 655 conservation-related publications compared five topics: climate change, habitat restoration, population viability analysis, land planning (systematic conservation planning) and invasive species. We analysed temporal changes in the: (i) questions asked by dispersal-related research; (ii) methods used to study dispersal; (iii) the quality of dispersal data; (iv) extent that dispersal knowledge is lacking, and; (v) likely consequences of limited dispersal knowledge. Research questions have changed little over time; the same problems examined in the 1990s are still being addressed. The most common methods used to study dispersal were occupancy data, expert opinion and modelling, which often provided indirect, low quality information about dispersal. Although use of genetics for estimating dispersal has increased, new ecological and genetic methods for measuring dispersal are not yet widely adopted. Almost half of the papers identified knowledge gaps related to dispersal. Limited dispersal knowledge often made it impossible to discover ecological processes or compromised conservation outcomes. The quality of dispersal data used in climate change research has increased since the 1990s. In comparison, restoration ecology inadequately addresses large-scale process, whilst the gap between knowledge accumulation and growth in applications may be increasing in land planning. To overcome apparent stagnation in collection and use of dispersal knowledge, researchers need to: (i) improve the quality of available data using new approaches; (ii) understand the complementarities of different methods and; (iii) define the value of different kinds of dispersal information for supporting management decisions. Ambitious, multi-disciplinary research programs studying many species are critical for advancing dispersal research.},
	language = {en},
	number = {4},
	urldate = {2025-06-27},
	journal = {PLOS ONE},
	author = {Driscoll, Don A. and Banks, Sam C. and Barton, Philip S. and Ikin, Karen and Lentini, Pia and Lindenmayer, David B. and Smith, Annabel L. and Berry, Laurence E. and Burns, Emma L. and Edworthy, Amanda and Evans, Maldwyn J. and Gibson, Rebecca and Heinsohn, Rob and Howland, Brett and Kay, Geoff and Munro, Nicola and Scheele, Ben C. and Stirnemann, Ingrid and Stojanovic, Dejan and Sweaney, Nici and Villaseñor, Nélida R. and Westgate, Martin J.},
	month = apr,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Biodiversity, Conservation biology, Climate change, Genetics, Conservation science, Ecosystems, Habitats, Invasive species},
	pages = {e95053},
}

@article{smith_dispersal_2023,
	title = {Dispersal inference from population genetic variation using a convolutional neural network},
	volume = {224},
	issn = {1943-2631},
	url = {https://doi.org/10.1093/genetics/iyad068},
	doi = {10.1093/genetics/iyad068},
	abstract = {The geographic nature of biological dispersal shapes patterns of genetic variation over landscapes, making it possible to infer properties of dispersal from genetic variation data. Here, we present an inference tool that uses geographically distributed genotype data in combination with a convolutional neural network to estimate a critical population parameter: the mean per-generation dispersal distance. Using extensive simulation, we show that our deep learning approach is competitive with or outperforms state-of-the-art methods, particularly at small sample sizes. In addition, we evaluate varying nuisance parameters during training—including population density, demographic history, habitat size, and sampling area—and show that this strategy is effective for estimating dispersal distance when other model parameters are unknown. Whereas competing methods depend on information about local population density or accurate inference of identity-by-descent tracts, our method uses only single-nucleotide-polymorphism data and the spatial scale of sampling as input. Strikingly, and unlike other methods, our method does not use the geographic coordinates of the genotyped individuals. These features make our method, which we call “disperseNN,” a potentially valuable new tool for estimating dispersal distance in nonmodel systems with whole genome data or reduced representation data. We apply disperseNN to 12 different species with publicly available data, yielding reasonable estimates for most species. Importantly, our method estimated consistently larger dispersal distances than mark-recapture calculations in the same species, which may be due to the limited geographic sampling area covered by some mark-recapture studies. Thus genetic tools like ours complement direct methods for improving our understanding of dispersal.},
	number = {2},
	urldate = {2025-06-27},
	journal = {Genetics},
	author = {Smith, Chris C R and Tittes, Silas and Ralph, Peter L and Kern, Andrew D},
	month = jun,
	year = {2023},
	pages = {iyad068},
}

@article{rousset_genetic_1997,
	title = {Genetic {Differentiation} and {Estimation} of {Gene} {Flow} from {F}-{Statistics} {Under} {Isolation} by {Distance}},
	volume = {145},
	issn = {1943-2631},
	url = {https://doi.org/10.1093/genetics/145.4.1219},
	doi = {10.1093/genetics/145.4.1219},
	abstract = {I reexamine the use of isolation by distance models as a basis for the estimation of demographic parameters from measures of population subdivision. To that aim, I first provide results for values of F-statistics in one-dimensional models and coalescence times in two-dimensional models, and make more precise earlier results for F-statistics in two-dimensional models and coalescence times in one-dimensional models. Based on these results, I propose a method of data analysis involving the regression of FST/ (1 – FST) estimates for pairs of subpopulations on geographic distance for populations along linear habitats or logarithm of distance for populations in two-dimensional habitats. This regression provides in principle an estimate of the product of population density and second moment of parental axial distance. In two cases where comparison to direct estimates is possible, the method proposed here is more satisfactory than previous indirect methods.},
	number = {4},
	urldate = {2025-06-27},
	journal = {Genetics},
	author = {Rousset, François},
	month = apr,
	year = {1997},
	pages = {1219--1228},
}

@article{browning_identity_2012,
	title = {Identity by descent between distant relatives: detection and applications},
	volume = {46},
	issn = {1545-2948},
	shorttitle = {Identity by descent between distant relatives},
	doi = {10.1146/annurev-genet-110711-155534},
	abstract = {Short segments of identity by descent (IBD) between individuals with no known relationship can be detected using genome-wide single nucleotide polymorphism data and recently developed statistical methodology. Emerging applications for the detected IBD segments include IBD mapping, haplotype phase inference, genotype imputation, and inference of population structure. In this review, we explain the principles behind methods for IBD segment detection, describe recently developed methods, discuss approaches to comparing methods, and give an overview of applications.},
	language = {eng},
	journal = {Annual Review of Genetics},
	author = {Browning, Sharon R. and Browning, Brian L.},
	year = {2012},
	pmid = {22994355},
	keywords = {Genetics, Population, Haplotypes, Humans, Pedigree, Polymorphism, Single Nucleotide, Computer Simulation, Linkage Disequilibrium, Computational Biology, Gene Frequency, Alleles, Chromosome Mapping, Chromosomes, Human, Genetic Diseases, Inborn, Genetic Predisposition to Disease, Genome-Wide Association Study, Inheritance Patterns},
	pages = {617--633},
}

@article{molder_sustainable_2021,
	title = {Sustainable data analysis with {Snakemake}},
	volume = {10},
	issn = {2046-1402},
	doi = {10.12688/f1000research.29032.2},
	abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way. Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
	language = {eng},
	journal = {F1000Research},
	author = {Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and Köster, Johannes},
	year = {2021},
	pmid = {34035898},
	pmcid = {PMC8114187},
	keywords = {Software, data analysis, Reproducibility of Results, adaptability, reproducibility, scalability, sustainability, transparency, workflow management, Data Analysis, Workflow},
	pages = {33},
}

@article{zhou_fast_2020,
	title = {A {Fast} and {Simple} {Method} for {Detecting} {Identity}-by-{Descent} {Segments} in {Large}-{Scale} {Data}},
	volume = {106},
	issn = {1537-6605},
	doi = {10.1016/j.ajhg.2020.02.010},
	abstract = {Segments of identity by descent (IBD) are used in many genetic analyses. We present a method for detecting identical-by-descent haplotype segments in phased genotype data. Our method, called hap-IBD, combines a compressed representation of haplotype data, the positional Burrows-Wheeler transform, and multi-threaded execution to produce very fast analysis times. An attractive feature of hap-IBD is its simplicity: the input parameters clearly and precisely define the IBD segments that are reported, so that program correctness can be confirmed by users. We evaluate hap-IBD and four state-of-the-art IBD segment detection methods (GERMLINE, iLASH, RaPID, and TRUFFLE) using UK Biobank chromosome 20 data and simulated sequence data. We show that hap-IBD detects IBD segments faster and more accurately than competing methods, and that hap-IBD is the only method that can rapidly and accurately detect short 2-4 centiMorgan (cM) IBD segments in the full UK Biobank data. Analysis of 485,346 UK Biobank samples through the use of hap-IBD with 12 computational threads detects 231.5 billion autosomal IBD segments with length ≥2 cM in 24.4 h.},
	language = {eng},
	number = {4},
	journal = {American Journal of Human Genetics},
	author = {Zhou, Ying and Browning, Sharon R. and Browning, Brian L.},
	month = apr,
	year = {2020},
	pmid = {32169169},
	pmcid = {PMC7118582},
	keywords = {Sequence Analysis, DNA, Genetics, Population, Genome, Human, Haplotypes, Humans, Polymorphism, Single Nucleotide, Computer Simulation, Software, Chromosomes, Alleles, Genetic Markers, Data Analysis, Genotype, haplotype, identity by descent, UK Biobank},
	pages = {426--437},
}

@article{browning_improving_2013,
	title = {Improving the accuracy and efficiency of identity-by-descent detection in population data},
	volume = {194},
	issn = {1943-2631},
	doi = {10.1534/genetics.113.150029},
	abstract = {Segments of indentity-by-descent (IBD) detected from high-density genetic data are useful for many applications, including long-range phase determination, phasing family data, imputation, IBD mapping, and heritability analysis in founder populations. We present Refined IBD, a new method for IBD segment detection. Refined IBD achieves both computational efficiency and highly accurate IBD segment reporting by searching for IBD in two steps. The first step (identification) uses the GERMLINE algorithm to find shared haplotypes exceeding a length threshold. The second step (refinement) evaluates candidate segments with a probabilistic approach to assess the evidence for IBD. Like GERMLINE, Refined IBD allows for IBD reporting on a haplotype level, which facilitates determination of multi-individual IBD and allows for haplotype-based downstream analyses. To investigate the properties of Refined IBD, we simulate SNP data from a model with recent superexponential population growth that is designed to match United Kingdom data. The simulation results show that Refined IBD achieves a better power/accuracy profile than fastIBD or GERMLINE. We find that a single run of Refined IBD achieves greater power than 10 runs of fastIBD. We also apply Refined IBD to SNP data for samples from the United Kingdom and from Northern Finland and describe the IBD sharing in these data sets. Refined IBD is powerful, highly accurate, and easy to use and is implemented in Beagle version 4.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {Browning, Brian L. and Browning, Sharon R.},
	month = jun,
	year = {2013},
	pmid = {23535385},
	pmcid = {PMC3664855},
	keywords = {Models, Genetic, Genetics, Population, Haplotypes, Humans, Pedigree, Polymorphism, Single Nucleotide, Finland, United Kingdom, Algorithms, Population, Beagle, identity-by-descent (IBD) segments, Sensitivity and Specificity, shared haplotypes},
	pages = {459--471},
}

@article{bezanson_julia_2017,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	abstract = {This is the third in a series of papers on aspects of modern computing environments that are relevant to statistical data analysis. In this paper, we discuss programming environments. In particular, we argue that integrated programming environments (for example, Lisp and Smalltalk environments) are more appropriate as a base for data analysis than conventional operating systems (for example, Unix).},
	number = {1},
	urldate = {2025-06-30},
	journal = {SIAM Review},
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
	month = jan,
	year = {2017},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {65--98},
}

@inproceedings{ge_turing_2018,
	title = {Turing: {A} {Language} for {Flexible} {Probabilistic} {Inference}},
	shorttitle = {Turing},
	url = {https://proceedings.mlr.press/v84/ge18b.html},
	abstract = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for building MCMC algorithms for probabilistic programming inference. Turing has a very simple syntax and makes full use of the numerical capabilities in the Julia programming language, including all implemented probability distributions, and automatic differentiation. Turing supports a wide range of popular Monte Carlo algorithms, including Hamiltonian Monte Carlo (HMC), HMC with No-U-Turns (NUTS), Gibbs sampling, sequential Monte Carlo (SMC), and several particle MCMC (PMCMC) samplers. Most importantly, Turing inference is composable: it combines MCMC operations on subsets of variables, for example using a combination of an HMC engine and a particle Gibbs (PG) engine.  We explore several combinations of inference methods with the aim of finding approaches that are both efficient and universal, i.e. applicable to arbitrary probabilistic models. NUTS—a popular variant of HMC that adapts Hamiltonian simulation path length automatically, although quite powerful for exploring differentiable target distributions, is however not universal. We identify some failure modes for the NUTS engine, and demonstrate that composition of PG (for discrete variables) and NUTS (for continuous variables) can be useful when the NUTS engine is either not applicable, or simply does not work well. Our aim is to present Turing and its composable inference engines to the world and encourage other researchers to build on this system to help advance the field of probabilistic machine learning.},
	language = {en},
	urldate = {2025-06-30},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1682--1690},
}
@misc{quadgk,
  title = {{QuadGK.jl}: {G}auss--{K}ronrod integration in {J}ulia},
  author = {Steven G. Johnson},
  year = {2013},
  howpublished = {\url{https://github.com/JuliaMath/QuadGK.jl}}
}
