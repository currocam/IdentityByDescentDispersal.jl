var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"H. Ringbauer, G. Coop and N. H. Barton. Inferring Recent Demography from Isolation by Distance of Long Shared Sequence Blocks. Genetics 205, 1335–1351 (2017). Accessed on May 5, 2024.\n\n\n\nB. C. Haller, J. Galloway, J. Kelleher, P. W. Messer and P. L. Ralph. Tree-sequence recording in SLiM opens new horizons for forward-time simulation of whole genomes. Molecular ecology resources 19, 552–566 (2019). Accessed on Feb 5, 2025.\n\n\n\n","category":"section"},{"location":"95-reference/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"API reference","title":"Contents","text":"Pages = [\"95-reference.md\"]","category":"section"},{"location":"95-reference/#Index","page":"API reference","title":"Index","text":"Pages = [\"95-reference.md\"]","category":"section"},{"location":"95-reference/#IdentityByDescentDispersal.age_density_ibd_blocks_custom","page":"API reference","title":"IdentityByDescentDispersal.age_density_ibd_blocks_custom","text":"age_density_ibd_blocks_custom(t::Real, r::Real, De::Function, parameters::AbstractArray, sigma::Real, L::Real, G::Real, chromosomal_edges::Bool = true, diploid::Bool = true)\n\nComputes the expected density of identity-by-descent (IBD) blocks of length L and age t for a model where the effective population density is given by a custom function De(t, parameters). This function returns the expected number of IBD blocks of age t per pair of individuals and per unit of block length.\n\nt is the age of the IBD block,\nr is the geographic distance between samples,\nDe is a user-defined function that takes time t and a parameters and returns the effective population density at time t.\nparameters is a user-defined array of parameters that the function De depends on.\nsigma is the root mean square dispersal distance per generation,\nL is the length of the IBD block (in Morgans),\nG is the total map length of the genome (in Morgans),\n\nIf chromosomal_edges is true (the default), we account for chromosomal edge effects. If diploid is true, we multiply by a factor of 4 to account for the fact that each individual has two copies of each chromosome.\n\nWe calculate it as\n\nmathbbEN_L^t =  G  4t^2 exp(-2Lt) cdot Phi(t)\n\nwhere phi(t) is the probability that two homologous loci coalesce t generations ago.\n\nReference: Ringbauer, H., Coop, G., & Barton, N. H. (2017). Genetics, 205(3), 1335–1351.\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#IdentityByDescentDispersal.composite_loglikelihood_constant_density-Tuple{Real, Real, DataFrames.DataFrame, AbstractArray{<:Real}}","page":"API reference","title":"IdentityByDescentDispersal.composite_loglikelihood_constant_density","text":"composite_loglikelihood_constant_density(D::Real, sigma::Real, df::DataFrame, contig_lengths::AbstractArray{<:Real}, chromosomal_edges::Bool=true, diploid::Bool=true) -> Real\n\nComputes the composite log-likelihood of the observed IBD blocks under a model with constant population density.\n\nD: Effective population density (diploid individuals per unit area).\nsigma: Root mean square dispersal distance per generation.\ndf: DataFrame containing the observed IBD blocks in the format returned by preprocess_dataset.\ncontig_lengths: Array of contig lengths in Morgans.\n\nOptionally:\n\nchromosomal_edges: Whether to account for chromosomal edge effects.\ndiploid: Whether to account for diploidy.\nverbose: Whether to print warnings when the computation fails.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.composite_loglikelihood_custom-Tuple{Function, AbstractArray, Real, DataFrames.DataFrame, AbstractArray{<:Real}}","page":"API reference","title":"IdentityByDescentDispersal.composite_loglikelihood_custom","text":"composite_loglikelihood_custom(De::Function, parameters::AbstractArray, sigma::Real, df::DataFrame, contig_lengths::AbstractArray{<:Real}, chromosomal_edges::Bool=true, diploid::Bool=true) -> Real\n\nComputes the composite log-likelihood of the observed IBD blocks under a model with constant population density.\n\nDe is  a user-defined function that takes time t and parameters and returns the effective population density at time t.\nparameters is a user-defined array of parameters that the function De depends on.\nsigma: Root mean square dispersal distance per generation.\ndf: DataFrame containing the observed IBD blocks in the format returned by preprocess_dataset.\ncontig_lengths: Array of contig lengths in Morgans.\n\nOptionally:\n\nchromosomal_edges: Whether to account for chromosomal edge effects.\ndiploid: Whether to account for diploidy.\nverbose: Whether to print warnings when the computation fails.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.composite_loglikelihood_power_density-Tuple{Real, Real, Real, DataFrames.DataFrame, AbstractArray{<:Real}}","page":"API reference","title":"IdentityByDescentDispersal.composite_loglikelihood_power_density","text":"composite_loglikelihood_power_density(D::Real, beta::Real, sigma::Real, df::DataFrame, contig_lengths::AbstractArray{<:Real}, chromosomal_edges::Bool=true, diploid::Bool=true) -> Real\n\nComputes the composite log-likelihood of the observed IBD blocks under a model with constant population density.\n\nD: Effective population density (diploid individuals per unit area).\nbeta is the power of the density function,\nsigma: Root mean square dispersal distance per generation.\ndf: DataFrame containing the observed IBD blocks in the format returned by preprocess_dataset.\ncontig_lengths: Array of contig lengths in Morgans.\n\nOptionally:\n\nchromosomal_edges: Whether to account for chromosomal edge effects.\ndiploid: Whether to account for diploidy.\nverbose: Whether to print warnings when the computation fails.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.default_ibd_bins-Tuple{}","page":"API reference","title":"IdentityByDescentDispersal.default_ibd_bins","text":"default_ibd_bins()\n\nReturns the default bins used in Ringbauer et. al. for identity-by-descent (IBD) block analysis, as well as the minimum length of IBD blocks to consider in Morgans.\n\nRingbauer, H., Coop, G. & Barton, N.H. Genetics 205, 1335–1351 (2017).\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.expected_ibd_blocks_constant_density","page":"API reference","title":"IdentityByDescentDispersal.expected_ibd_blocks_constant_density","text":"expected_ibd_blocks_constant_density(r::Real, D::Real, sigma::Real, L::Real, G::Real, chromosomal_edges::Bool=true, diploid::Bool=true) -> Real\n\nComputes the expected density of identity-by-descent (IBD) blocks of length L for a model with constant population density. This function returns the expected number of IBD blocks per pair of individuals and per unit of block length.\n\nr is the geographic distance between samples,\nD is the effective population density (diploid individuals per unit area),\nsigma is the root mean square dispersal distance per generation,\nL is the length of the IBD block (in Morgans),\nG is the total map length of the genome (in Morgans)\n\nIf chromosomal_edges is true (the default), we account for chromosomal edge effects. If diploid is true, we multiply by a factor of 4 to account for the fact that each individual has two copies of each chromosome. There is a function overload that accepts a vector of G values and returns the aggregated expected number of IBD blocks.\n\nWe calculate it as\n\nmathbbEN_L = int_0^infty mathbbEN_L^t dt =\nfracG8pi D sigma^2\nleft(fracrsqrtL sigmaright)^2\nK_2left(fracsqrt2L  rsigmaright)\n\nwhere K₂ is the modified Bessel function of the second kind of order 2. Reference: Ringbauer, H., Coop, G., & Barton, N. H. (2017). Genetics, 205(3), 1335–1351.\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#IdentityByDescentDispersal.expected_ibd_blocks_custom","page":"API reference","title":"IdentityByDescentDispersal.expected_ibd_blocks_custom","text":"expected_ibd_blocks_custom(r::Real, De::Function, parameters::AbstractArray, sigma::Real, L::Real, G::Real, chromosomal_edges::Bool = true, diploid::Bool = true)\n\nComputes the expected density of identity-by-descent (IBD) blocks of length L for a model where the effective population density is given by a custom function De(t, parameters). This function returns the expected number of IBD blocks per pair of individuals and per unit of block length.\n\nr is the geographic distance between samples,\nDe is  a user-defined function that takes time t and a parameters and returns the effective population density at time t.\nparameters is a user-defined array of parameters that the function De depends on.\nsigma is the root mean square dispersal distance per generation,\nL is the length of the IBD block (in Morgans),\nG is the total map length of the genome (in Morgans),\n\nIf chromosomal_edges is true (the default), we account for chromosomal edge effects. If diploid is true, we multiply by a factor of 4 to account for the fact that each individual has two copies of each chromosome. There is a function overload that accepts a vector of G values and returns the aggregated expected number of IBD blocks.\n\nWe calculate it as\n\nmathbbEN_L = int_0^infty mathbbEN_L^t dt = int_0^infty G  4t^2 exp(-2Lt) cdot Phi(t) dt\n\nwhere the integral is computed numerically using Gaussian-Legendre quadrature rules with the QuadGK package.\n\nReference: Ringbauer, H., Coop, G., & Barton, N. H. (2017). Genetics, 205(3), 1335–1351.\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#IdentityByDescentDispersal.expected_ibd_blocks_power_density","page":"API reference","title":"IdentityByDescentDispersal.expected_ibd_blocks_power_density","text":"expected_ibd_blocks_power_density(r::Real, D::Real, beta::Real, sigma::Real, L::Real, G::Real, chromosomal_edges::Bool = true, diploid::Bool = true)\n\nComputes the expected density of identity-by-descent (IBD) blocks of length L for a model with a power population density in the form of D(t) = D_0t^-eta This function returns the expected number of IBD blocks per pair of individuals and per unit of block length.\n\nr is the geographic distance between samples,\nD is the effective population density (diploid individuals per unit area),\nbeta is the power of the density function,\nsigma is the root mean square dispersal distance per generation,\nL is the length of the IBD block (in Morgans),\nG is the total map length of the genome (in Morgans),\n\nIf chromosomal_edges is true (the default), we account for chromosomal edge effects. If diploid is true, we multiply by a factor of 4 to account for the fact that each individual has two copies of each chromosome. There is a function overload that accepts a vector of G values and returns the aggregated expected number of IBD blocks.\n\nWe calculate it as\n\nmathbbEN_L = int_0^infty mathbbEN_L^t dt =\n2^frac-3beta2-3\nfracGpi D sigma^2\nleft(fracrsqrtL sigmaright)^(2+beta)\nK_2+betaleft(fracsqrt2L  rsigmaright)\n\nwhere K₂ is the modified Bessel function of the second kind of order 2. Reference: Ringbauer, H., Coop, G., & Barton, N. H. (2017). Genetics, 205(3), 1335–1351.\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#IdentityByDescentDispersal.preprocess_dataset-Tuple{DataFrames.DataFrame, DataFrames.DataFrame, AbstractVector, Real}","page":"API reference","title":"IdentityByDescentDispersal.preprocess_dataset","text":"preprocess_dataset(ibd_blocks::DataFrame, individual_distances::DataFrame, bins::AbstractVector, min_length::Real)\n\nPreprocesses the input data for identity-by-descent (IBD) block analysis.\n\nibd_blocks: DataFrame containing IBD blocks with columns ID1, ID2, and span. The ID1 and ID2 columns should contain the IDs of the individuals involved in the IBD block, and the span column should contain the length of the IBD block in Morgans.\nindividual_distances: DataFrame containing distances between individuals with columns ID1, ID2, and distance. The ID1 and ID2 columns should contain the IDs of the individuals involved in the distance. Notice that the units of the estimated density and dispersal rate will match the units of the distances provided.\nbins: A vector of right bins for the IBD blocks.\nmin_length: Minimum length of IBD blocks to consider in Morgans.\n\nIt returns a DataFrame in \"long\" format with the following columns:\n\nDISTANCE: The pairwise distance between individuals.\nIBD_LEFT: The left bound of the IBD length bin.\nIBD_RIGHT: The right bound of the IBD length bin.\nIBD_MID: The center of the IBD length bin.\nNR_PAIRS: The number of unique individual pairs within that distance.\nCOUNT: The number of IBD blocks observed in the corresponding bin.\nDISTANCE_INDEX: The index of the distance bin.\nIBD_INDEX: The index of the IBD length bin.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.probability_coalescence-Tuple{Real, Real, Function, Real}","page":"API reference","title":"IdentityByDescentDispersal.probability_coalescence","text":"probability_coalescence(t::Real, r::Real, De::Function, sigma::Real)\n\nComputes the probability phi(t) that two homologous loci coalesce t generations ago according to\n\nphi(t) = frac12D_e(t) frac14pi t sigma^2 exp(frac-r^24 t sigma^2)\n\nRingbauer, H., Coop, G. & Barton, N.H. Genetics 205, 1335–1351 (2017).\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#IdentityByDescentDispersal.safe_adbesselk-Tuple{Any, Any}","page":"API reference","title":"IdentityByDescentDispersal.safe_adbesselk","text":"x = safe_adbesselk(1, 1.0)\n\nA wrapper around the adbesselk to compute the modified Bessel function of the second kind of degree that returns a NaN if computation fails.\n\n\n\n\n\n","category":"method"},{"location":"overview/#Theory-overview","page":"Theory overview","title":"Theory overview","text":"This package provides an efficient implementation of the inference scheme proposed by [1] to estimate the mean dispersal rate and the effective population density of a population.  Here, we present an overview of the theory behind the method, but we refer to the original publication for details.\n\nMates tend to live close to each other and their offspring. This results in an inverse correlation between geographical distance and genetic relatedness. The goal of this package is to do demographic inference based on such spatial genetic patterns.","category":"section"},{"location":"overview/#Diffusion-approximation-of-dispersal","page":"Theory overview","title":"Diffusion approximation of dispersal","text":"First, let’s provide some intuition on how we model the dispersal of individuals in continuous space.  From an ecological perspective, we might be interested in modeling the details of single-generation dispersal, such as the mating process or differences in dispersal across individuals and sexes.  Such details are, however, not generally feasible to estimate from genetic data. Instead, we aim to estimate the mean dispersal rate, this is for a given gene in a given individual the expected euclidean distance to the parent from which the gene was inherited.\n\nThe method implemented here approximates the spatial movement of genetic material using a diffusion process. Let r(t) be the distance between two homologous gene copies at a time t before the present. First, note that when t=textTMRCA (time to the most common recent ancestor), then r(textTMRCA) = 0. The physical distance between both homologs at the present, r(0)=r_textobs (i.e. the distance we can observe), is the sum of the sequence of displacements due to dispersal since the time to the most common recent ancestor.\n\nIf dispersal distances are independent and identically distributed, then, by the central-limit theorem, the random variable r_textobs converges to a Gaussian distribution with axial variance of sigma^2times textTMRCA as textTMRCA grows. Here sigma^2 is the average squared axial parent-offspring distance (i.e. the dispersal rate).\n\nThis result is independent of the details of the single-generation dispersal process, as long as the variance of the dispersal kernel is finite. For example, let’s consider a population where dispersal across a single-generation dispersal can be modeled using a uniform distribution. Below, we visualize the distribution of distances between homologous loci when t = 1 (single-generation dispersal) and t=30.  For simplicity, we assume 1-dimensional space. As we can see, the approximation becomes very accurate even after a few generations.\n\nusing Distributions, Plots, StatsPlots\ndisplacement = Uniform(-1, 1)\nσ2 = var(displacement) # Theoretical variance of displacement\ndistance(t) = sum(rand(displacement, t)); # Distance after t migrations\nn_draws = 10_000\np1 = histogram(\n    [distance(1) for _ = 1:n_draws],\n    normalize = true,\n    label = \"t = 1\",\n    xlabel = \"Distance\",\n    ylabel = \"Density\",\n)\nplot!(p1, Normal(0, sqrt(σ2 * 1)), lw = 2, label = \"Gaussian Approximation\")\n\np2 = histogram(\n    [distance(30) for _ = 1:n_draws],\n    normalize = true,\n    label = \"t = 30\",\n    xlabel = \"Distance\",\n    ylabel = \"Density\",\n)\nplot!(p2, Normal(0, sqrt(σ2 * 30)), lw = 2, label = \"Gaussian Approximation\")\nplot(p1, p2, layout = (1, 2), size = (900, 400), legend = :topright, framestyle = :box)\n\nThis approximation is expected to work well in regimes where the time to the most common recent ancestor is not extremely small and when dispersal is not correlated across generations and is time-homogeneous (i.e. displacements are independent and identically distributed).\n\nFrom the previous section, it is clear that it would be possible to design an inference scheme based on the diffusion approximation if we knew the exact time to the most recent common ancestor between homologs. For example, the probability of two homologs being more than one unit apart given that they coalesced 20 generations ago can be calculated from the density of the Gaussian approximation for any sigma.\n\n1 - cdf(Normal(0, sqrt(σ2 * 20)), 1)","category":"section"},{"location":"overview/#Coalescent-theory","page":"Theory overview","title":"Coalescent theory","text":"However, the time to the most common recent ancestor is not directly observable. Instead, what we can do is marginalize across all possible times 0 infty). Here one can use standard coalescent theory to model the time to coalescence. Roughly speaking, we consider a scenario where two homologous loci might coalesce when they become very close. The rate with which they coalesce when they are very close is inversely proportional to local effective density D_e (effective number of individuals per unit of space).  According to this model, the coalescence rate at time t before the present for two homologs that are currently a distance d apart is p(r(t)=0r(0)=d) times frac1D_e(t), i.e. the rate at which they come close enough times the rate at which they coalesce given that they are close enough.\n\ngenerations = 1:300\nD_e = 1 # Constant effective density of 1 (individuals per unit area)\nσ = 1 # Constant dispersal rate of 1( distance units per generation)\nr = 3 # Pairwise distance between loci\ndensities = [pdf(Normal(0, sqrt(σ^2 * t)), r) / D_e for t in generations]\nplot(\n    generations,\n    densities,\n    ylabel = \"Coalescence rate ϕ(t)\",\n    xlabel = \"Generations\",\n    label = \"(D=1, σ=1, r=3)\",\n)","category":"section"},{"location":"overview/#IBD-blocks","page":"Theory overview","title":"IBD blocks","text":"The last ingredient of this method is the relationship between identity-by-descent (IBD) blocks and time to the most recent common ancestor. First, we point out that generally only recent coalescent events are informative of the dispersal rate. This is the reason why this inference scheme relies on identity-by-descent blocks, as such data are more informative about the recent past than polymorphism data.\n\nWe say that a segment of DNA is a shared identity-by-descent block if it has been inherited from a common ancestor without being broken up by recombination. A shared identity-by-descent block of length L that finds its common ancestor at time t has \"survived\" exactly 2t meiosis events. Therefore, we expect larger blocks to be more recent, as L decays with t.\n\nIf we model recombination as a Poisson process and measure genetic distance in Morgans (as is typically done), then the probability that a region of length L does not recombine can be obtained from the exponential distribution with rate exp(-2Lt)\n\n[1] derived that the expected density of identity-by-descent blocks of length L per pair of haploid individuals and per unit of block length given that the time to the most recent common ancestor is t is given by\n\nEK_L  t approx G 4 t^2 exp(-2Lt)\n\nwhere G is the length of the genome in Morgans. This equation is obtained by accounting for the probability that there is a region of length L that has not recombined (the exp(-2Lt) term) which is delimited by two recombination events (the 4 t^2 term) and summing across all possible start sites (the G factor). A slightly more complex expression that accounts for the effect of chromosomal edges and diploidy is provided in the article and the software implementation.","category":"section"},{"location":"overview/#Inference-scheme","page":"Theory overview","title":"Inference scheme","text":"As mentioned above, the inference scheme relies on computing the expected density of identity-by-descent blocks under a given demographic model. This involves marginalizing across all possible times of coalescence, therefore computing integrals in the form of\n\nmathbbEL = int_0^infty mathbbEL  t phi(t) dt\n\nwhere phi(t) is the probability density function of the time to the most recent common ancestor. This expression is hard to track down analytically. [1] derived an analytical solution for a family of demographic models. Alternatively, this software implementation provides a numerical approximation of the integral using Gaussian-quadrature rules. We refer to the other sections of the documentation for more details.\n\n[1] proposed an inference scheme where they assumed the number of observed shared identity-by-descent blocks whose length fall in a small bin follows a Poisson distribution. The rate of the distribution can be calculated from the expected density of identity-by-descent blocks under the demographic model of interest. If the bin is small enough, it can be calculated as\n\nmathbbEL L+Delta L = mathbbEL Delta L\n\nFinally, they propose to approximate the likelihood of the observed data across many bins and pairs of individuals using a composite likelihood (that is, by assuming pairwise observations are independent).\n\nL(theta  Y_i^j) = Pr(K = Y_i^j  mathbbEL_i^j Delta L)\n\nwhere Y_i^j is the number of observed shared-identity-by-descent blocks whose length fall in the ith bin and are shared by the jth pair of individuals. They then define the composite likelihood as\n\nCL(theta  textData) = prod_ij Pr(K = Y_i^j  mathbbEL_i^j Delta L)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#IdentityByDescentDispersal.jl","page":"Home","title":"IdentityByDescentDispersal.jl","text":"IdentityByDescentDispersal.jl is a Julia package for estimating effective population densities and dispersal rates from observed spatial patterns of IBD shared blocks. It provides the building blocks for performing likelihood-based inference and integrates seamlessly with other statistical libraries in the Julia ecosystem, such as Turing.jl.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"You can install this package by running:\n\nimport Pkg\nPkg.add(\"IdentityByDescentDispersal\")","category":"section"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"This website contains the documentation for IdentityByDescentDispersal.\n\nThe documentation is organised into several sections:\n\nTheory overview: An informal description of the theory behind the package, which covers the main concepts (e.g. dispersal rates, IBD blocks) and assumptions of the model.\nBasic usage: A comprehensive guide to the functions provided by the package, to be used both for inference and prediction.\nInference and model evaluation: A complete inference example with simulated data and a set of recommendations for automated data processing (using a Snakemake pipeline) and model evaluation.\nAPI reference: A complete reference of the functions exported by the package.","category":"section"},{"location":"inference/#Inference-and-model-evaluation","page":"Inference and model evaluation","title":"Inference and model evaluation","text":"In this section, we go through the process of estimating the parameters of the model and evaluating the fitted model.","category":"section"},{"location":"inference/#Simulating-synthetic-datasets","page":"Inference and model evaluation","title":"Simulating synthetic datasets","text":"The method proposed by [1] makes some assumptions to approximate the evolution of a population in a continuous space. We refer to the Theory overview for an introduction to the underlying theory. In any case, we advise researchers to simulate synthetic data that resembles the empirical data according to their expertise. This is a crucial step to determine the expected performance of the method and to guide different steps of the pre-processing strategy (such as which software to use to detect identity-by-descent blocks or how to bin the observed data).\n\nFor this purpose, we provide a SLiM [2] model to simulate an individual-based population in a continuous space as well as two scripts that fit the model with either error-free identity-by-descent blocks or blocks detected using state-of-the-art software and compare it with the ground-truth parameters. Such models can be found at simulations directory and might be adapted to your specific needs.","category":"section"},{"location":"inference/#Processing-pipeline","page":"Inference and model evaluation","title":"Processing pipeline","text":"Inferring the effective density and dispersal rate from empirical datasets requires running an identity-by-descent detection software across different phased VCF files, post-processing them, and aggregating them appropriately. This procedure is error-prone and time-consuming.\n\nTo facilitate the analysis, we provide a Snakemake pipeline that can be used to perform a complete analysis, from detecting IBD blocks using HapIBD, post-processing them with Refined IBD, producing a CSV directly compatible with this package, and, optionally, finding a preliminary maximum likelihood estimate of the effective density and effective dispersal rate. Snakemake is a popular tool in bioinformatics and it allows easy parallelization across multiple cores or jobs when submitting to a cluster via SLURM, as well as automatically managing dependencies via Conda or Apptainer containers. Most of the time, running such a pipeline would require a single-command such as:\n\nsnakemake -s Snakefile --configfile config.yaml --cores 8 --sdm conda\n\nThe pipeline is configured via a configuration YAML file. An example of such a configuration file can be found here. You can also find a toy compatible dataset at the .test-workflow subdirectory We refer to the extensive documentation of Snakemake for more information.","category":"section"},{"location":"inference/#Model-inference","page":"Inference and model evaluation","title":"Model inference","text":"Next, we exemplify the common steps of an analysis using IdentityByDescentDispersal. First, we load a synthetic dataset for which we know the ground truth. This is the dataset we simulated when showcasing how to simulate synthetic datasets using SLiM.\n\nusing JLD2, DataFrames, IdentityByDescentDispersal, Turing, Random\nRandom.seed!(1234)\n\nLoad the synthetic dataset from the package's docs/data directory\n\ndata = load(\n    joinpath(pkgdir(IdentityByDescentDispersal), \"docs\", \"data\", \"constant_density.jld2\"),\n)\nground_truth = (data[\"local_density\"], data[\"dispersal_rate\"])\n\nThe IdentityByDescentDispersal is designed to be compatible with existing statistical software. Here, we decide to Use the Turing package, which is the most popular Bayesian modelling framework in Julia. Let's consider the following model:\n\n@model function constant_density(df, contig_lengths)\n    De ~ Truncated(Normal(1000, 100), 0, Inf)\n    σ ~ Truncated(Normal(1, 0.1), 0, Inf)\n    Turing.@addlogprob! composite_loglikelihood_constant_density(\n        De,\n        σ,\n        df,\n        contig_lengths,\n        verbose = false,\n    )\nend\nm = constant_density(data[\"df\"], data[\"contig_lengths\"])\n\nThe IdentityByDescentDispersal is compatible with automatic differentiation. Therefore, we can use standard Hamiltonian Monte Carlo algorithms such as NUTS() to estimate the posterior distribution.\n\nchains = sample(m, NUTS(), MCMCThreads(), 500, 4; progress = false);\nnothing #hide\n\nWhen fitting models with power-densities (i.e. using composite_loglikelihood_power_density), be aware that the beta parameter controls the mean-square diﬀerentiability of the composite likelihood (through modified second-kind Bessel function). If you run into issues, we suggest that:\n\nreparametrize the model (perhaps using a custom function via the composite_loglikelihood_custom interface).\ntighten the priors to constrain the space of parameters to a region where gradients are defined.\nuse gradient-free inference methods such as Nelder-Mead when computing maximum likelihood estimates or Metropolis-Hastings when doing Bayesian inference.","category":"section"},{"location":"inference/#Model-diagnosis","page":"Inference and model evaluation","title":"Model diagnosis","text":"Diagnosing the posterior samples obtained via Markov Chain Monte Carlo is a crucial step in any Bayesian inference. We refer to existing resources such as this lecture on Bayesian modelling in Biostatistics. Most popular approaches involve calculating quantities such as the effective number of samples (ESS) and hat R, which can be computed directly from Turing output.\n\nAs a rule of thumb, we aim to run the chain long enough to obtain an ESS greater than 100.\n\ness(chains) |> DataFrame\n\nA hat R greater than 1.05 indicates the chains have not mixed well.\n\nrhat(chains) |> DataFrame\n\nConvergence issues can also be inspected through a traceplot:\n\nusing Plots, StatsPlots\ntraceplot(chains)","category":"section"},{"location":"inference/#Visualizing-the-posterior","page":"Inference and model evaluation","title":"Visualizing the posterior","text":"After finishing the sampling process and assessing convergence, we can visualize the estimated posterior distribution of the parameters and compare it with the prior distribution and the ground-truth:\n\nusing Distributions\np1 = plot(\n    Truncated(Normal(1000, 100), 0, Inf),\n    label = \"Prior\",\n    xlab = \"Effective population density\",\n    ylab = \"Density\",\n)\ndensity!(p1, chains[:De][:], label = \"Posterior\")\nvline!(p1, [ground_truth[1]], label = \"Ground-truth\")\n\np2 = plot(\n    Truncated(Normal(1, 0.1), 0, Inf),\n    label = \"Prior\",\n    xlab = \"Effective dispersal rate\",\n    ylab = \"Density\",\n)\ndensity!(p2, chains[:σ][:], label = \"Posterior\")\nvline!(p2, [ground_truth[2]], label = \"Ground-truth\")\nplot(p1, p2, layout = (2, 1), size = (600, 800))\n\nNotice that, although the inference is accurate, we do not expect the posterior estimates to have nominal coverage (e.g., that a 90% credibility interval contains the true parameter 90% of the time). This is because we are assuming every pairwise observation is independent when constructing the composite likelihood and therefore we are overconfident in our estimations. In their publication, [1] computed a bootstrapping confidence interval of the maximum-likelihood estimate but still do not achieve nominal coverage.\n\nA bootstrapping confidence interval can be easily computed with the maximum_likelihood function and taking samples with replacement of the rows of the DataFrame. Notice, however, that other resampling schemes are possible. We advise you to experiment with different approaches and evaluate the performance with simulated datasets.\n\nnboots = 100\nboots = zeros(2, nboots)\ndf = data[\"df\"]\nfor i = 1:nboots\n    df_resampled = df[sample(1:nrow(df), nrow(df), replace = true), :]\n    mle = maximum_likelihood(constant_density(df_resampled, data[\"contig_lengths\"]))\n    boots[:, i] = mle.values\nend\nconf_De = quantile(boots[1, :], [0.025, 0.975])\nconf_σ = quantile(boots[2, :], [0.025, 0.975])\nDataFrame(\n    parameter = [\"De\", \"σ\"],\n    confidence_interval95 = [conf_De, conf_σ],\n    ground_truth = [ground_truth[1], ground_truth[2]],\n)","category":"section"},{"location":"inference/#Interpreting-the-output","page":"Inference and model evaluation","title":"Interpreting the output","text":"Finally, we provide some references on how to interpret the results and relate them to the biological context.","category":"section"},{"location":"inference/#Effective-population-density","page":"Inference and model evaluation","title":"Effective population density","text":"The effective population density (De) is simply the inverse of the coalescent rate of lineages that become very close to each other. It is equivalent to the effective population size of every deme in a stepping stone model where demes are separated by one distance unit.\n\nThe estimate is most informative of recent demographic events as it is calculated from long identity-by-descent blocks that typically arise recently. We can get an idea of the temporal time-scale by looking at the theoretical predictions under the estimated demographic model.\n\nIn a Bayesian setting, this sort of information is known as a posterior predictive distribution. Next, we provide a snippet of code that demonstrates how to do this provided a MCMC sample.\n\nlet\n    grid_times = 1:100\n    L = 0.04 # Smallest IBD block considered\n    grid_r = [0.1, 0.5, 1.0] # Geographic distances\n    colors = [:blue, :red, :green] # Define colors for each r\n    De(t, params) = params[1] # Custom De(t) parametrization\n    p1 = plot(xlabel = \"Time (generations ago)\", ylabel = \"Density shared IBD blocks\")\n    posterior = sample(chains, 100)\n    for (i, r) in enumerate(grid_r)\n        first_line = true\n        for (D, sigma) in zip(posterior[:De], posterior[:σ])\n            params = [D]\n            dens = age_density_ibd_blocks_custom(\n                grid_times,\n                r,\n                De,\n                params,\n                sigma,\n                L,\n                data[\"contig_lengths\"],\n            )\n            plot!(\n                p1,\n                grid_times,\n                dens,\n                label = first_line ? \"Distance=$r\" : \"\",\n                color = colors[i],\n                alpha = 0.1,\n            )\n            first_line = false\n        end\n    end\n    p1\nend\n\nAn equivalent plot can be done in a likelihood (frequentist) setting. For example, by calculating the densities for each bootstrap replicate.\n\nlet\n    grid_times = 1:100\n    L = 0.04 # Smallest IBD block considered\n    grid_r = [0.1, 0.5, 1.0] # Geographic distances\n    colors = [:blue, :red, :green] # Define colors for each r\n    De(t, params) = params[1] # Custom De(t) parametrization\n    p1 = plot(xlabel = \"Time (generations ago)\", ylabel = \"Density shared IBD blocks\")\n    for (i, r) in enumerate(grid_r)\n        first_line = true\n        for j = 1:nboots\n            D, sigma = boots[:, j]\n            params = [D]\n            dens = age_density_ibd_blocks_custom(\n                grid_times,\n                r,\n                De,\n                params,\n                sigma,\n                L,\n                data[\"contig_lengths\"],\n            )\n            plot!(\n                p1,\n                grid_times,\n                dens,\n                label = first_line ? \"Distance=$r\" : \"\",\n                color = colors[i],\n                alpha = 0.1,\n            )\n            first_line = false\n        end\n    end\n    p1\nend\n\nWe expect to observe a decay in the density of expected shared IBD-blocks with time. The previous plot can be used to determine some time interval (1 t) from which we expect most of the signal to have arisen. This recipe is useful for identifying the time-span of the estimated demography and can be applied to other demographic models as well.","category":"section"},{"location":"inference/#Effective-dispersal-rate","page":"Inference and model evaluation","title":"Effective dispersal rate","text":"Perhaps unintuitively, the mean per-generation dispersal distance when we average with respect to both parents is not necessarily equivalent to the mean per-generation dispersal distance when we average across single generations. From population genetic data, we can estimate the latter ( refer to the Theory overview section for more details). Therefore, interpreting the estimated effective dispersal rate in the context of the ecology of one population has to be done carefully.\n\nFor example, consider a population with two separate sexes for which there are differences in the process of dispersal. Such differences arise naturally when considering the effect of mating systems on dispersal patterns. For simplicity, let's consider a 1-dimensional space where individuals can only move left or right as shown in the figure below.\n\n(Image: Illustration of dispersal in a 1-dimensional space with separate sexes)\n\nLet's assume the displacement between the mother and the offspring, d_textmother-child, is distributed according to a normal distribution with a mean of zero and a variance of sigma_D^2 and the displacement between the father and the mother, d_textmother-father, is also distributed according to a normal distribution with a mean of zero and a variance of sigma_M^2. Because the mating process (that determines the displacement from the mother to the father) is independent from the specific dispersal process of the offspring (that determines the displacement from the mother to the offspring), then the displacement between the father and the offspring is distributed according to a normal distribution with a mean of zero and a variance of sigma_D^2 + sigma_M^2.\n\nAs mentioned earlier, what we can estimate is the mean per-generation dispersal of the \"lineages\". Because the lineage is either inherited from the mother or the father, the displacement is distributed as a mixture of two normal distributions. If sex ratio is 0.5, then the displacement is distributed as a mixture of two normal distributions with weights w = 05 05. This distribution has a mean of zero and a variance of sigma_D^2 + 05sigma_M^2 but it is not normally distributed. Therefore, and under these assumptions, what we can estimate from genetic data is actually sigma = sqrt05sigma_D^2 + 05(sigma_D^2+sigma_M^2) = sqrtsigma_D^2 + 05sigma_M^2) and it is not possible to separate the effects of sigma_D and sigma_M without additional information. If a priori information is available, it is possible to obtain estimates of both sigma_D and sigma_M that are consistent with the observed data. However, this requires additional consideration as the posterior distribution will be multimodal.\n\n@model function constant_density2(df, contig_lengths)\n    De ~ Truncated(Normal(1000, 100), 0, Inf)\n    σ_D ~ Truncated(Normal(1, 0.1), 0, Inf)\n    σ_M ~ Truncated(Normal(0.1, 0.01), 0, Inf)\n    σ := sqrt(σ_D^2 + 0.5 * σ_M^2)\n    Turing.@addlogprob! composite_loglikelihood_constant_density(De, σ, df, contig_lengths)\nend\n\nAs mentioned earlier, the per-generation distance averaging across lineages is not necessarily equal to the mean per-generation distance when we average with respect to both mother and father. Notice that the average displacement is defined as hat d = 05 d_textmother-child + 05 (d_textmother-child - d_textmother-father) which is distributed as a normal distribution with mean zero and variance sqrtsigma_D^2 + 025 sigma_M^2.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorial/#Basic-usage","page":"Basic usage","title":"Basic usage","text":"This section demonstrates the basic usage of the package. We refer to the Inference and model evaluation section of the documentation for an explanation of how to use this package to estimate parameters of demographic models from observed data.\n\nusing IdentityByDescentDispersal","category":"section"},{"location":"tutorial/#Expected-density-of-identity-by-descent-blocks","page":"Basic usage","title":"Expected density of identity-by-descent blocks","text":"Genetic distances between populations are often a function of geographic distance. This phenomenon is often referred to as isolation-by-distance. Isolation-by-distance is reflected in the patterns of identity-by-descent blocks (also known as IBD blocks). It is possible to derive the expected patterns of shared identity-by-descent blocks under a diffusion approximation and certain demographic models. We refer to the publication of [1] and the Theory overview section of the documentation for more details.\n\nThe main feature of this package is to compute the expected density of identity-by-descent blocks under different demographic scenarios. Next, we show how to compute the expected density of identity-by-descent blocks under a constant population density using the expected_ibd_blocks_constant_density function.\n\nusing Plots\nusing QuadGK\n\nL = 0.01         # Block length threshold (in Morgans)\nG = 1.0          # Genome length (in Morgans)\nr_values = range(0.01, 25.0, length = 200);  # Distances\n\nplot(\n    r_values,\n    expected_ibd_blocks_constant_density.(r_values, 1.0, 0.5, L, G),\n    xlabel = \"Geographic distance between individuals (r)\",\n    ylabel = \"E[#IBD blocks per unit of block length and per pair]\",\n    title = \"Constant effective density scenario\",\n    label = \"D=1.0, σ=0.5\",\n)\nplot!(\n    r_values,\n    expected_ibd_blocks_constant_density.(r_values, 2.0, 0.5, L, G),\n    label = \"D=2.0, σ=0.5\",\n)\nplot!(\n    r_values,\n    expected_ibd_blocks_constant_density.(r_values, 1.0, 0.8, L, G),\n    label = \"D=1.0, σ=0.8\",\n)\n\nThe figure above shows how the density of IBD blocks of length 1 centimorgan in a genome of 1 Morgan decays as a function of geographic distance for different demographic scenarios that vary in effective density and dispersal rate. The effective density is in units of number of diploid individuals per area unit (e.g., km²). The dispersal rate is defined as the square root of the average squared axial parent-offspring distance. The rate of the decay of identity-by-descent blocks is directly related to the time since the most recent common ancestor.\n\nRecall that the length of IBD blocks is a continuous random variable. Therefore, the expected number of IBD blocks of exactly length L is zero. What the expected_ibd_blocks_constant_density function returns is the expected number of IBD blocks of length L per unit of block length (in Morgans) and per pair. In order to compare such expectations with observed data, we often want to consider instead the expected number of blocks whose length falls in a given interval [a, b]. For a small enough interval L L + Delta L we can approximate the expected number of blocks whose length falls in that interval as:\n\nEN_L L + Delta L = int_L^L + Delta L EN_L dL approx EN_L Delta L\n\nFor example, the expected number of identity-by-descent blocks whose length falls in the interval [1cM, 1.1cM] shared by two diploid individuals that are 1 kM apart in a population with constant effective population density of 10/kM²and dispersal rate of 0.5/kM would be\n\nexpected_ibd_blocks_constant_density(1.0, 10.0, 0.5, L, G) * 0.001\n\nHowever, we can also compute the expected number of blocks for any bin size using numerical integration (and we do this automatically when computing log-likelihoods internally if necessary):\n\nquadgk(x -> expected_ibd_blocks_constant_density(x, 10.0, 0.5, L, G) * 0.001, 1.0, 1.1)[1]\n\n[1] derived analytical solutions for the expected density of identity-by-descent blocks for the family of functions of effective population density of the form D_e(t) = D_1 t^-beta. A constant effective population density is a special case of this family with beta = 0. For other values of beta, the expected density of identity-by-descent blocks can be computed using the expected_ibd_blocks_power_density function.\n\nIn addition to power density functions, we also support arbitrary effective density functions provided by the user via numerical integration. Next, we illustrate how to define a custom function by considering the popular choice of an exponential growth function.\n\nD = 1.0          # Effective population density\nσ = 1.0          # Dispersal rate\nfunction De(t, θ)\n    D₀, α = θ\n    max(D₀ * exp(-α * t), eps())\nend\nplot(\n    r_values,\n    [expected_ibd_blocks_custom(r, De, [D, 0.0], σ, L, G) for r in r_values],\n    xlabel = \"Geographic distance between individuals (r)\",\n    ylabel = \"E[#IBD blocks per unit of block length and per pair]\",\n    title = \"Constant effective density scenario\",\n    label = \"growth rate α = 0.0\",\n)\nplot!(\n    r_values,\n    [expected_ibd_blocks_custom(r, De, [D, 0.005], σ, L, G) for r in r_values],\n    label = \"growth rate α = 0.005\",\n)\nplot!(\n    r_values,\n    [expected_ibd_blocks_custom(r, De, [D, -0.005], σ, L, G) for r in r_values],\n    label = \"growth rate α = -0.005\",\n)\n\nIt is possible to define more complex effective density functions. However, users should consider carefully whether such parameters are identifiable from identity-by-descent blocks if they aim to estimate them. As an example, we compare the one scenario with an oscillating effective density function with a constant effective density demography.\n\nfunction De(t, θ)\n    D₀, a, ω = θ\n    D₀ * (1 + a * sin(ω * t))\nend\nD = 1.0          # Effective population density\nσ = 1.0          # Dispersal rate\nθ = [D, 0.5, 2π]  # Parameters for De(t): D₀, a, ω\nt_values = range(0.0, 10.0, length = 200)   # Time for plotting D_e(t)\np1 = plot(\n    t_values,\n    [De(t, θ) for t in t_values],\n    xlabel = \"Time (generations ago)\",\n    ylabel = \"Effective population density\",\n    label = \"Oscillating effective density\",\n)\nhline!(p1, [D], label = \"Constant effective density\")\np2 = plot(\n    r_values,\n    [expected_ibd_blocks_custom(r, De, θ, σ, L, G) for r in r_values],\n    xlabel = \"Geographic distance between individuals (r)\",\n    ylabel = \"E[#IBD blocks per unit of block length and per pair]\",\n    label = \"Oscillating effective density\",\n)\nplot!(\n    p2,\n    r_values,\n    [expected_ibd_blocks_constant_density(r, D, σ, L, G) for r in r_values],\n    label = \"Constant effective density\",\n)\n\nplot(p1, p2, layout = (2, 1), size = (600, 800))","category":"section"},{"location":"tutorial/#Inference","page":"Basic usage","title":"Inference","text":"Ringbauer et. al (2017) proposed to do inference by assuming that the observed number of IBD blocks that a pair r units apart that fall within a small bin L_i L_i + Delta L follows a Poisson distribution with mean EN_L_i(r theta) Delta L.\n\nTherefore, the log probability of observing Y identity-by-descent blocks whose length fall in the bin L_i L_i + Delta L from a pair of individuals that are r units apart can be calculated simply as:\n\nusing Distributions\ndelta_L = 0.001\nλ = expected_ibd_blocks_constant_density(\n    0.2, # Distance,\n    0.5, # Dispersal rate\n    2.0, # Effective density\n    0.01, # Block length threshold (in Morgans)\n    1.0, # Genome length (in Morgans)\n) * delta_L\nY = 5 # Observed number of IBD blocks\nlogpdf(Poisson(λ), Y)\n\nThe inference scheme is based on a composite-likelihood that treats each bin from different pairs of individuals as independent. For computational reasons, it is also faster to aggregate observations that correspond to the same distance and bin.\n\nThe input data for this sort of analysis is:\n\nA DataFrame containing the length of identity-by-descent blocks shared across different individuals.\nA DataFrame containing distances across pairs of individuals.\nA list of contig lengths to properly take into account chromosomal edges (in Morgans).\n\nusing DataFrames\nibd_blocks = DataFrame(\n    ID1 = [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    ID2 = [\"B\", \"A\", \"C\", \"A\", \"C\"],\n    span = [0.1, 0.06, 0.09, 0.11, 0.08], # Morgans\n)\nindividual_distances = DataFrame(\n    ID1 = [\"A\", \"A\", \"B\"],\n    ID2 = [\"B\", \"C\", \"C\"],\n    distance = [1, 1.5, 2], # (e.g., in kilometers)\n)\ncontig_lengths = [0.2, 0.1]; # Contig lengths (in Morgans)\nnothing #hide\n\nPreprocessing the data requires combining the different sources of information and binning the identity-by-descent blocks. For this purpose, we provide a helper function preprocess_dataset that returns a DataFrame in a \"long\" format. Of course, you may specify your own bins, but here we use the same bins used by [1], which can be accessed via the default_ibd_bins function.\n\nbins, min_length = default_ibd_bins()\ndf = preprocess_dataset(ibd_blocks, individual_distances, bins, min_length);\nnothing #hide\n\nIn most scenarios, distances between diploid individuals are not available, but between sampling sites. The preprocess_dataset function handles this scenario appropriately by aggregating observations from pairs that have the exact same distance. However, if distances are available at the individual level, you may consider binning them to reduce the runtime. You might find an example of this in the simulations subdirectory.\n\nWe can now use the df DataFrameto compute composite log likelihoods of different parameters using the family of composite_loglikelihood_* functions. They will automatically choose an appropriate integration method based on the binning of the data. The second major feature of this package is that functions are all compatible with automatic differentiation. Analytical solutions depend on a modified Bessel function of the second kind, which is not widely implemented in a way that allows for automatic differentiation . Here, we rely on the BesselK.jl package for doing this. In addition, we rely on the system of QuadGK.jl for numerical integration compatible with automatic differentiation. This allows us to perform gradient-based optimization and interact with existing software.\n\nWe recommend using this package together with the Turing.jl, as it provides a convenient interface for Bayesian and frequentist inference.","category":"section"},{"location":"tutorial/#Maximum-likelihood-estimation","page":"Basic usage","title":"Maximum likelihood estimation","text":"For example, we can estimate the parameters of a constant density model using maximum likelihood estimation:\n\nusing Turing, StatsBase, StatsPlots, Random\n\nRandom.seed!(1000)\n@model function constant_density(df, contig_lengths)\n    D ~ Uniform(0, 1000)\n    σ ~ Uniform(0, 1000)\n    Turing.@addlogprob! composite_loglikelihood_constant_density(D, σ, df, contig_lengths)\nend\n\nGenerate a MLE estimate.\n\nmle_estimate = maximum_likelihood(constant_density(df, contig_lengths))\ncoefs = DataFrame(coeftable(mle_estimate)); # computed from the Fisher information matrix\ncoefs[!, [\"Name\", \"Coef.\", \"Std. Error\", \"Lower 95%\", \"Upper 95%\"]]","category":"section"},{"location":"tutorial/#Bayesian-inference","page":"Basic usage","title":"Bayesian inference","text":"Alternatively, we can do a standard Bayesian inference with any of the available inference algorithms such as NUTS. Here, we fit a power-density model using NUTS:\n\n@model function power_density(df, contig_lengths)\n    D ~ Truncated(Normal(100, 20), 0, Inf)\n    σ ~ Truncated(Normal(1, 0.1), 0, Inf)\n    β ~ Normal(0, 0.5)\n    Turing.@addlogprob! composite_loglikelihood_power_density(D, β, σ, df, contig_lengths)\nend\nm = power_density(df, contig_lengths)\n\nSample 4 chains in a serial fashion.\n\nchains = sample(m, NUTS(), MCMCSerial(), 1000, 4; progress = false)\nplot(chains)\n\nMore complex models can be built with cautious consideration of identifiability. For example, we can fit a piecewise exponential density model using a custom function and numerical integration.\n\nfunction piecewise_D(t, parameters)\n    De1, De2, alpha, t0 = parameters\n    t <= t0 ? De1 * exp(-alpha * t) : De2\nend\n@model function exponential_density(df, contig_lengths)\n    De1 ~ Truncated(Normal(1000, 100), 0, Inf)\n    De2 ~ Truncated(Normal(1000, 100), 0, Inf)\n    α ~ Normal(0, 0.05)\n    t0 ~ Truncated(Normal(500, 100), 0, Inf)\n    σ ~ Truncated(Normal(1, 0.1), 0, Inf)\n    theta = [De1, De2, α, t0]\n    Turing.@addlogprob! composite_loglikelihood_custom(\n        piecewise_D,\n        theta,\n        σ,\n        df,\n        contig_lengths,\n    )\nend\nm = exponential_density(df, contig_lengths);\nnothing #hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
